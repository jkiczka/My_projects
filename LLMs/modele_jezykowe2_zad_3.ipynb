{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "model_name = \"flax-community/papuGaPT2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\")\n",
        "\n",
        "def log_probs_from_logits(logits, labels):\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "    return logp_label\n",
        "\n",
        "\n",
        "\n",
        "def sentence_prob(sentence_txt):\n",
        "    input_ids = tokenizer(sentence_txt, return_tensors='pt')['input_ids'].to(\"cpu\")\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids=input_ids)\n",
        "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
        "        seq_log_probs = torch.sum(log_probs)\n",
        "    return seq_log_probs.cpu().numpy()\n",
        "\n",
        "\n",
        "def beam_search(variants, beam_width=3):\n",
        "    # beam search inicjalizuje beam jako pustą listę\n",
        "    beam = [(0, \"\")]  # każdy element to (prawdopodobieństwo, sekwencja)\n",
        "\n",
        "    for word_variants in variants:\n",
        "        new_beam = []\n",
        "\n",
        "        # przeglądanie każdej ścieżki w beam\n",
        "        for prob, seq in beam:\n",
        "            for variant in word_variants:\n",
        "                # tworzenie nowego tekstu na podstawie wariantu\n",
        "                new_seq = f\"{seq} {variant}\".strip()\n",
        "                new_prob = sentence_prob(new_seq)\n",
        "\n",
        "                # dodajemy nową ścieżkę do beam\n",
        "                new_beam.append((new_prob, new_seq))\n",
        "\n",
        "\n",
        "        beam = sorted(new_beam, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "\n",
        "    return beam[0][1]\n",
        "\n",
        "def main(input_string):\n",
        "    words_variants = [group.split('|') for group in input_string.split() if group]\n",
        "    best_sequence = beam_search(words_variants)\n",
        "    return best_sequence\n",
        "\n",
        "\n",
        "input_string = \"\"\"\\\n",
        "wprost|wyprosty|wyprostu|wyprost uwielbiała|wielbił|wielbiła|uwielbił|wielbiło|uwielbiał|uwielbiało|uwielbiały\n",
        "słuchać|osłuchać|słychać|usłuchać o|i|e|a|ó|ę|y|ą|u\n",
        "wartościach własnych|owłosionych macierzy|mocarz|macierzą|macierze|mocarza|mocarze|mocarzy|macierz\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "output = main(input_string)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Br8C2mwZPY",
        "outputId": "2d13f32d-0f54-4f1f-e678-0b6a8d3a13f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wprost uwielbiał słuchać o wartościach własnych macierzy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=\"Słońce|Końce|Mące świeciło|świergotało|świeżbiło wysoko|wysoki|wyskoki na niebie|niedzielę|niewiele, kiedy|kły|kłody zobaczyliśmy|zboczyliśmy|zmoczyliśmy domek|bobek|tomek na horyzoncie|froncie|koncie\"\n",
        "print(main(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BpUThQYz-3Y",
        "outputId": "e60530bb-3b03-4721-e485-95175b83c0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Słońce świeciło wysoko na niebie kiedy zobaczyliśmy domek na horyzoncie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"Słonko|Słońce|Słowik świeciło|śpiewało|świszczało nad stawem|statkiem|strachem, gdy|gładko|gniewnie zauważyliśmy|zanotowaliśmy|zasłyszeliśmy że|ze|z zaczyna|zmoczyła|zawita się|śpię|szły robić|rozbić|rodzić ciemno|cienko|ciężko\"\n",
        "print(main(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hFkq7PjHs7M",
        "outputId": "99488f17-8759-4a9f-8ea1-768d239252f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Słońce świeciło nad stawem gdy zauważyliśmy że zaczyna się robić ciemno\n"
          ]
        }
      ]
    }
  ]
}