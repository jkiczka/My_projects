{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "def tokenize(word):\n",
        "    ids = tokenizer(word, return_tensors='pt')['input_ids'][0]\n",
        "    return [tokenizer.decode(n) for n in ids]\n",
        "\n",
        "def cos(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "model_name = 'flax-community/papuGaPT2'\n",
        "device = 'cpu'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "clusters = {}\n",
        "with open('categories.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        category, words = line.strip().split(\":\")\n",
        "        word_list = words.split()\n",
        "        clusters[category] = word_list\n",
        "\n",
        "embeddings = model.transformer.wte.weight.detach().cpu().numpy()\n",
        "N = embeddings.shape[0]\n",
        "\n",
        "word_embeddings = {}\n",
        "\n",
        "for category, words in clusters.items():\n",
        "    for w in words:\n",
        "        tokens = tokenize(' ' + w)\n",
        "        token_ids = tokenizer.encode(tokens[0])\n",
        "\n",
        "        word_embedding = np.zeros(embeddings.shape[1])\n",
        "\n",
        "        for token_id in token_ids:\n",
        "            word_embedding += embeddings[token_id]\n",
        "\n",
        "        word_embedding /= len(token_ids)\n",
        "\n",
        "        word_embeddings[w] = word_embedding\n",
        "\n",
        "with open('papuga.txt', 'w') as f:\n",
        "    for word, embedding in word_embeddings.items():\n",
        "        embedding_str = ' '.join(map(str, embedding))\n",
        "        f.write(f\"{word} {embedding_str}\\n\")\n"
      ],
      "metadata": {
        "id": "C3l9IS8E_tRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "device = 'cpu'\n",
        "model_name = \"allegro/herbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "def tokenize(word):\n",
        "    ids = tokenizer(word, return_tensors='pt')['input_ids'][0]\n",
        "    return ids.detach().numpy()\n",
        "\n",
        "def embedding_raw(words):\n",
        "    txt = ' '.join(words)\n",
        "    input_ids = tokenizer(txt, return_tensors='pt')['input_ids'].to(device)\n",
        "    output = model(input_ids=input_ids)\n",
        "    return output.last_hidden_state.detach().cpu().numpy()[0, 0, :]\n",
        "\n",
        "def embedding(word, ctx):\n",
        "    return embedding_raw([word] + ctx)\n",
        "\n",
        "def random_typo(word):\n",
        "    word = list(word)\n",
        "    idx = random.randint(0, len(word)-1)\n",
        "    word[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "    return ''.join(word)\n",
        "\n",
        "def transposition_typo(word):\n",
        "    word = list(word)\n",
        "    if len(word) < 2:\n",
        "        return ''.join(word)\n",
        "    idx1 = random.randint(0, len(word)-1)\n",
        "    idx2 = random.randint(0, len(word)-1)\n",
        "    while idx1 == idx2:\n",
        "        idx2 = random.randint(0, len(word)-1)\n",
        "\n",
        "    word[idx1], word[idx2] = word[idx2], word[idx1]\n",
        "    return ''.join(word)\n",
        "\n",
        "def process_clusters():\n",
        "    clusters = {}\n",
        "    with open('categories.txt', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            category, words = line.strip().split(\":\")\n",
        "            word_list = words.split()\n",
        "            clusters[category] = word_list\n",
        "\n",
        "\n",
        "    with open('bert.txt', 'w') as file:\n",
        "        for category, words in clusters.items():\n",
        "            for word in words:\n",
        "                emb = embedding(word, words)\n",
        "                file.write(f\"{word} \")\n",
        "                for e in emb:\n",
        "                    file.write(f\"{e} \")\n",
        "                file.write(\"\\n\")\n",
        "\n",
        "    with open('bert_random.txt', 'w') as f:\n",
        "        for category, words in clusters.items():\n",
        "            for word in words:\n",
        "                word_with_random_typo = random_typo(word)\n",
        "                emb = embedding(word_with_random_typo, words)\n",
        "                embedding_with_random_typo_str = ' '.join(map(str, emb))\n",
        "                f.write(f\"{word} {embedding_with_random_typo_str}\\n\")\n",
        "\n",
        "    with open('bert_transposition.txt', 'w') as f:\n",
        "        for category, words in clusters.items():\n",
        "            for word in words:\n",
        "                word_with_transposition_typo = transposition_typo(word)\n",
        "                emb = embedding(word_with_transposition_typo, words)\n",
        "                embedding_with_transposition_typo_str = ' '.join(map(str, emb))\n",
        "                f.write(f\"{word} {embedding_with_transposition_typo_str}\\n\")\n",
        "\n",
        "process_clusters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hgbsWcfB-Vu",
        "outputId": "abc11f06-f7a7-4a69-ac63-31e19efe96c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "device = 'cpu'\n",
        "model_name = \"allegro/herbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "def tokenize(word):\n",
        "    ids = tokenizer(word, return_tensors='pt')['input_ids'][0]\n",
        "    return ids.detach().numpy()\n",
        "\n",
        "def embedding_raw(words):\n",
        "    txt = ' '.join(words)\n",
        "    input_ids = tokenizer(txt, return_tensors='pt')['input_ids'].to(device)\n",
        "    output = model(input_ids=input_ids)\n",
        "    return output.last_hidden_state.detach().cpu().numpy()[0, 0, :]\n",
        "\n",
        "def embedding(word, ctx):\n",
        "    return embedding_raw([word] + ctx)\n",
        "\n",
        "def process_clusters(result_file_name):\n",
        "    clusters = {}\n",
        "    with open('categories.txt', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            category, words = line.strip().split(\":\")\n",
        "            word_list = words.split()\n",
        "            clusters[category] = word_list\n",
        "\n",
        "    with open(result_file_name, 'w') as file:\n",
        "        for category, words in clusters.items():\n",
        "            print(category)\n",
        "            for word in words:\n",
        "                emb = embedding(word, words)  # Use entire category as context\n",
        "                file.write(f\"{word} \")\n",
        "                for e in emb:\n",
        "                    file.write(f\"{e} \")\n",
        "                file.write(\"\\n\")\n",
        "\n",
        "process_clusters('bert.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdEcsTTQAQjY",
        "outputId": "0e4d63ef-0fd2-4395-e958-f6942c5347ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "piśmiennicze\n",
            "małe_ssaki\n",
            "okręty\n",
            "lekarze\n",
            "zupy\n",
            "uczucia\n",
            "działy_matematyki\n",
            "budynki_sakralne\n",
            "stopień_wojskowy\n",
            "grzyby_jadalne\n",
            "prądy_filozoficzne\n",
            "religie\n",
            "dzieła_muzyczne\n",
            "cyfry\n",
            "owady\n",
            "broń_biała\n",
            "broń_palna\n",
            "komputery\n",
            "kolory\n",
            "duchowny\n",
            "ryby\n",
            "napoje_mleczne\n",
            "czynności_sportowe\n",
            "ubranie\n",
            "mebel\n",
            "przestępca\n",
            "mięso_wędliny\n",
            "drzewo\n",
            "źródło_światła\n",
            "organ\n",
            "oddziały\n",
            "napój_alkoholowy\n",
            "kot_drapieżny\n",
            "metal\n",
            "samolot\n",
            "owoc\n",
            "pościel\n",
            "agd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S37TyvzE7S9l",
        "outputId": "98a77b28-b81f-420f-f172-a215dfd3ac08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the cumulative distribution function (CDF) for the standard normal distribution at 0.9\n",
        "F_09 = 1-norm.cdf(-0.65)\n",
        "round(F_09, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyrTZPEujAJh",
        "outputId": "5509470c-456c-4881-ccdc-276938ddd1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.742"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}